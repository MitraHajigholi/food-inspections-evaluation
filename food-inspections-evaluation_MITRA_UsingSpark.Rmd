---
title: "Chicago Food Inspection Using Spark"
author: "Mitra Hajigholi"
date: "2 May 2018"
output: html_document
---


### Initialization
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(sparklyr)
library(dplyr)
library(ggplot2)

sc <- spark_connect("local")

```
### READ
```{r eval=FALSE}
dat_model <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/dat_model.Rds")
#dat_model_tbl <- copy_to(sc, dat_model[,-1], "dat_model", overwrite = TRUE)
dat_model_tbl <- copy_to(sc, select(dat_model,-Inspection_Date), "dat_model", overwrite = TRUE)

food_inspections <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/food_inspections.Rds")
# either remove the data column or change the datatype of the date into a string before copying it to spark
food_inspections_tbl <- copy_to(sc, food_inspections[,-11], "food_inspections", overwrite = TRUE)

burglary_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/burglary_heat.Rds")
burglary_heat_tbl <- copy_to(sc, burglary_heat, "burglary_heat", overwrite = TRUE)

garbageCarts_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/garbageCarts_heat.Rds")
garbageCarts_heat_tbl <- copy_to(sc, garbageCarts_heat, "garbageCarts_heat", overwrite = TRUE)

sanitationComplaints_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/sanitationComplaints_heat.Rds")
sanitationComplaints_heat_tbl <- copy_to(sc, sanitationComplaints_heat, "sanitationComplaints_heat", overwrite = TRUE)


```



##==============================================================================
## Options for working with spark results sets
##==============================================================================
- Use lazy execution to construct statements and get the results on the fly
- Use sdf_register() to put the results into a spark table (update)
- Use sdf_persist() to force any pending calcs to happen - doesn't necessarily persist in a nicely named object
- Use collect() to perform the calcs and bring the results into an R data.frame

to tell spark to register our changes to do stuff with it, run this
 sparklyr::sdf_register("nameoftable")
 or 
 collect() brings into R memory



### Exploratory
```{r eval=FALSE}

head(food_inspections_tbl)  # head works for tables and dataframes

# Only works with dataframe, not table as spark use. need to change the template dataexplorer uses to gerate the report into a spark version. 

#dat_model %>% 
#  as_data_frame() %>% 
#  DataExplorer::GenerateReport()  


#food_inspections %>% 
#  as_data_frame() %>% 
#  DataExplorer::GenerateReport()  
```

```{r}

### Plot the heat map

# Spark can not plot, we need to collect the data intro R mode to plot it.


# ggplot(garbageCarts_heat_tbl, aes(x=heat_values)) + geom_density()
# 
# 
# food_inspections %>% 
#     inner_join(sanitationComplaints_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))
# 
# 
# 
# food_inspections %>% 
#     inner_join(garbageCarts_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))
# 
# 
# 
# food_inspections %>% 
#     inner_join(burglary_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))

```
### Reason for failing Inspection
- Burglary (broken)
- garbage complaints (dirty)
- sanitation complains (dirty)
- warm weather (higher risk of failiur) 
- humidity(!) + temperatureMax
- criticalCount + pastCritical (!)
- seriousCount + pastSerious   (!)
- past minor
- pastFail
- fail_flag
- mobile_food_license
- consumption_on_premises_incidental_activi (?)
- Public_place_of_amusement
- childrens_services_facility_license
- 




##==============================================================================
## INITIALIZE
##==============================================================================

```{r}

# Load libraries
#geneorama::detach_nonstandard_packages()
## Load libraries that are used
#geneorama::loadinstall_libraries(c("data.table", "glmnet", "ggplot2"))
## Load custom functions
#geneorama::sourceDir("CODE/functions/")


```
##==============================================================================
## MUTATE TABLES
##==============================================================================
```{r}

dat <- dat_model_tbl

### list contains Reason for failing Inspection
## Add criticalFound variable to dat:

dat  %>%
    rename(Inspector = Inspector_Assigned) %>% 
    mutate(criticalFound = pmin(1, criticalCount),
           pastSerious = pmin(pastSerious, 1),
           pastCritical = pmin(pastCritical, 1),
           ageAtInspection = ifelse(ageAtInspection > 4, 1L, 0L),
           heat_burglary = pmin(heat_burglary, 70),
           heat_sanitation = pmin(heat_sanitation, 70),
           heat_garbage = pmin(heat_garbage, 50)
           ) ->
    dat_mutated
    ## Set the key for dat


head(dat_mutated)

#mm <- model.matrix(criticalFound ~ . -1, data=xmat[ , -1, with=F])
#mm <- as.data.table(mm)
#str(mm)
#colnames(mm)


## Check to see if any rows didn't make it through the model.matrix formula
nrow(dat)
nrow(xmat)
#nrow(mm)

#setkey(dat, Inspection_ID)  # can not use set key in spark, do not need to...
head(dat_cleaned)
head(dat_mutated)
```


##==============================================================================
## FILTER and SELECT TABLES 
## -CREATE MODEL DATA
##==============================================================================
```{r}

## Only keep "Retail Food Establishment"
dat_mutated  %>%  
    filter(LICENSE_DESCRIPTION == "Retail Food Establishment") %>% 
    select(Inspector,
           pastSerious,
           criticalFound,
           pastCritical,
           ageAtInspection,
           -LICENSE_DESCRIPTION, 
           pastFail,
           fail_flag,
           timeSinceLast,
           consumption_on_premises_incidental_activity,
           childrens_services_facility_license,
           public_place_of_amusement,
           mobile_food_license,
           tobacco_retail_over_counter,
           temperatureMax,
           humidity,
           heat_burglary,
           heat_sanitation,
           heat_garbage,
           criticalFound
           )   ->   ## Remove License Description    
    dat_cleaned


####Check
## print nr of row     
sdf_nrow(dat_cleaned)
sdf_nrow(dat)
# and columns
sdf_ncol(dat_cleaned)
sdf_ncol(dat)

```






##==============================================================================
## CREATE TEST / TRAIN PARTITIONS
##==============================================================================
## randomly select 70% train /30% test
```{r}

partitions <- 
  sdf_partition(dat_cleaned, training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
    select(criticalFound, pastSerious, heat_sanitation) %>% 
  ml_logistic_regression(criticalFound~.)

fit

sdf_predict(fit, partitions$test)
#partitions$test %>% sdf_predict(fit, .)

dat_cleaned %>% 
  modelr::resample_partition(c(train=0.7, test =0.3)) ->
  splits

splits %>% 
  pluck("train") %>% 
  as_data_frame() ->
  train_raw

splits %>% 
  pluck("test") %>% 
  as_data_frame() ->
  test_raw
  
  
#iiTrain <- train_raw
#iiTest <- test_raw

# sparklyr::sdf_register("nameoftable")

```

##==============================================================================
## Prepping data, Scaling of data 
##==============================================================================
```{r}


prepdata <- prep(recipe(criticalFound~., train_raw)  %>% 
                       step_naomit(all_predictors())) 
                        
prepdata <- prep(prepdata %>% 
                       step_BoxCox(temperatureMax, humidity) %>% 
                       step_YeoJohnson(heat_burglary, heat_sanitation, heat_garbage)
                   ) 

# do what we prepped to do
train_prep <- bake(prepdata, train_raw)
test_prep <- bake(prepdata, test_raw)

                        

                        

ggplot(data =train_prep, aes(x=train_prep$temperatureMax)) + 
  geom_density() 
 
ggplot(data =train_prep, aes(x=train_prep$humidity)) + 
  geom_density() 

ggplot(data =train_prep, aes(x=train_prep$heat_burglary)) + 
  geom_density() 

ggplot(data =train_prep, aes(x=train_prep$heat_sanitation)) + 
  geom_density() 

ggplot(data =train_prep, aes(x=train_prep$heat_garbage)) + 
  geom_density() 
                   
                    
                    
```
 


##==============================================================================
## A glmnet for feature selection
##==============================================================================

#Use regularization to smooth results by modifying coefficients of variables.

```{r}

# what scaling values, how good the models are as a result.
glmnet_unbal <- glmnet(criticalFound~.,
                       train_prep,
                       family="binomial",
                       alpha = 0.5,  # some l1 and ome l2
                       intercept = FALSE)

glmnet_unbal



glance(glmnet_unbal) # returns key metrics of the models fit 
```



```{r}
# scale of the coeff at each of the points
plot(glmnet_unbal, label = TRUE)  

# each line is a coeff, top x axisshows number of columns, L1Norm = lasso normalized??
```


```{r}

#set.seed(1050104) # reproduce random
# cv = crossvalidation, multiple iteration of the model fitting process, each iteration runs on each slice of the data (splits it up in 5 random samples, each iteration 4 used for training, 1 for testing) 
glmnet_unbal_cv <- cv.glmnet(criticalFound~.,
                             train_prep,
                             family="binomial",  #distribution family
                             alpha = 0.5)

plot(glmnet_unbal_cv)

# different regularization 
# many coulmns, penalty to columns = peanilizing, 
# the better we are predicting the outcome, the higher y axis will be = binomial deviance
# dotted lines are showing cut-off points for the lambda coeff, basically simlpe or complex good enough...

coefficients(glmnet_unbal_cv)

coef(glmnet_unbal_cv, s = "lambda.min")

#lambda.min = lambda that most minimises predictive error
#lambda.1se = lambda that performs more penalization but still has close predictive power

```



```{r}

test_raw %>% 
  bake(prepdata, .) %>% 
  modelr:: add_predictions(glmnet_unbal,var = "glm_unbal") ->
  test_scored

#test_scored %>% 
#  ggplot(aes(x=glmnet_unbal, group=was_delayed, fill= was_delayed)) + 
#  geom_density(alpha=0.5) + 
#  geom_vline(aes(xintercept=0))

test_scored$glmnet_unbal_cv <- as.vector(predict(glmnet_unbal_cv, 
                                                 test_scored,
                                                 na.action = na.pass)) 


test_scored$classification <- ifelse(test_scored$glmnet_unbal_cv < -2, 0,1) 


test_scored %>% 
    count(criticalFound,classification)

# gray = no critc failiur
# ljusbl = critical found
test_scored %>% 
  ggplot(aes(x=glmnet_unbal_cv, group=criticalFound, fill= criticalFound)) + 
  geom_density(alpha=0.5) + 
  geom_vline(aes(xintercept=-2))
```

