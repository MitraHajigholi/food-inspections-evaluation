---
title: "Chicago Food Inspection Using Spark"
author: "Mitra Hajigholi"
date: "2 May 2018"
output: html_document
---


### Initialization
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(sparklyr)
library(dplyr)
library(ggplot2)
library(ggthemes)

sc <- spark_connect("local")

```
### READ
```{r eval=FALSE}
dat_model <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/dat_model.Rds")
#dat_model_tbl <- copy_to(sc, dat_model[,-1], "dat_model", overwrite = TRUE)
dat_model_tbl <- copy_to(sc, select(dat_model,-Inspection_Date), "dat_model", overwrite = TRUE)

food_inspections <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/food_inspections.Rds")
# either remove the data column or change the datatype of the date into a string before copying it to spark
food_inspections_tbl <- copy_to(sc, food_inspections[,-11], "food_inspections", overwrite = TRUE)

burglary_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/burglary_heat.Rds")
burglary_heat_tbl <- copy_to(sc, burglary_heat, "burglary_heat", overwrite = TRUE)

garbageCarts_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/garbageCarts_heat.Rds")
garbageCarts_heat_tbl <- copy_to(sc, garbageCarts_heat, "garbageCarts_heat", overwrite = TRUE)

sanitationComplaints_heat <- readRDS("C:/Users/Admin/Documents/food-inspections-evaluation/DATA/sanitationComplaints_heat.Rds")
sanitationComplaints_heat_tbl <- copy_to(sc, sanitationComplaints_heat, "sanitationComplaints_heat", overwrite = TRUE)


```



##==============================================================================
## Options for working with spark results sets
##==============================================================================
- Use lazy execution to construct statements and get the results on the fly
- Use sdf_register() to put the results into a spark table (update)
- Use sdf_persist() to force any pending calcs to happen - doesn't necessarily persist in a nicely named object
- Use collect() to perform the calcs and bring the results into an R data.frame

to tell spark to register our changes to do stuff with it, run this
 sparklyr::sdf_register("nameoftable")
 or 
 collect() brings into R memory



### Exploratory
```{r eval=FALSE}

head(food_inspections_tbl)  # head works for tables and dataframes

# Only works with dataframe, not table as spark use. need to change the template dataexplorer uses to gerate the report into a spark version. 

#dat_model %>% 
#  as_data_frame() %>% 
#  DataExplorer::GenerateReport()  


#food_inspections %>% 
#  as_data_frame() %>% 
#  DataExplorer::GenerateReport()  
```

```{r}

### Plot the heat map

# Spark can not plot, we need to collect the data intro R mode to plot it.


# ggplot(garbageCarts_heat_tbl, aes(x=heat_values)) + geom_density()
# 
# 
# food_inspections %>% 
#     inner_join(sanitationComplaints_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))
# 
# 
# 
# food_inspections %>% 
#     inner_join(garbageCarts_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))
# 
# 
# 
# food_inspections %>% 
#     inner_join(burglary_heat) %>% 
#     ggplot(aes(x=Latitude, y= Longitude, colour=heat_values)) + 
#                geom_point(alpha=0.5, size=1) +
#                scale_color_gradientn(colours = viridisLite::viridis(256, option = "A"))

```
### Reason for failing Inspection
- Burglary (broken)
- garbage complaints (dirty)
- sanitation complains (dirty)
- warm weather (higher risk of failiur) 
- humidity(!) + temperatureMax
- criticalCount + pastCritical (!)
- seriousCount + pastSerious   (!)
- past minor
- pastFail
- fail_flag
- mobile_food_license
- consumption_on_premises_incidental_activi (?)
- Public_place_of_amusement
- childrens_services_facility_license
- 




##==============================================================================
## INITIALIZE
##==============================================================================

```{r}

# Load libraries
#geneorama::detach_nonstandard_packages()
## Load libraries that are used
#geneorama::loadinstall_libraries(c("data.table", "glmnet", "ggplot2"))
## Load custom functions
#geneorama::sourceDir("CODE/functions/")


```
##==============================================================================
## MUTATE TABLES
##==============================================================================
```{r}

dat <- dat_model_tbl

### list contains Reason for failing Inspection
## Add criticalFound variable to dat:

dat  %>%
    rename(Inspector = Inspector_Assigned) %>% 
    mutate(criticalFound = pmin(1, criticalCount),
           pastSerious = pmin(pastSerious, 1),
           pastCritical = pmin(pastCritical, 1),
           ageAtInspection = ifelse(ageAtInspection > 4, 1L, 0L),
           heat_burglary = pmin(heat_burglary, 70),
           heat_sanitation = pmin(heat_sanitation, 70),
           heat_garbage = pmin(heat_garbage, 50)
           ) ->
    dat_mutated
    ## Set the key for dat


head(dat_mutated)

#mm <- model.matrix(criticalFound ~ . -1, data=xmat[ , -1, with=F])
#mm <- as.data.table(mm)
#str(mm)
#colnames(mm)


## Check to see if any rows didn't make it through the model.matrix formula
nrow(dat)
nrow(dat_mutated)

#setkey(dat, Inspection_ID)  # can not use set key in spark, do not need to...
head(dat)
head(dat_mutated)
```


##==============================================================================
## FILTER and SELECT TABLES 
## -CREATE MODEL DATA
##==============================================================================
```{r}

## Only keep "Retail Food Establishment"
dat_mutated  %>%  
    filter(LICENSE_DESCRIPTION == "Retail Food Establishment") %>% 
    select(#Inspector,
           pastSerious,
           criticalFound,
           pastCritical,
           ageAtInspection,
           -LICENSE_DESCRIPTION, 
           pastFail,
           fail_flag,
           timeSinceLast,
           consumption_on_premises_incidental_activity,
           childrens_services_facility_license,
           public_place_of_amusement,
           mobile_food_license,
           tobacco_retail_over_counter,
           temperatureMax,
           humidity,
           heat_burglary,
           heat_sanitation,
           heat_garbage,
           criticalFound
           )   ->   ## Remove License Description    
    dat_cleaned


####Check
## print nr of row     
sdf_nrow(dat_cleaned)
sdf_nrow(dat)
# and columns
sdf_ncol(dat_cleaned)
sdf_ncol(dat)

```






##==============================================================================
## CREATE TEST / TRAIN PARTITIONS
##==============================================================================
## randomly select 70% train /30% test
```{r}

partitions <- 
  sdf_partition(dat_cleaned, training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
    select(criticalFound, pastSerious, heat_sanitation, pastFail, pastCritical) %>% 
  ml_logistic_regression(criticalFound~.)


scored_data <- sdf_predict(fit, partitions$test)
#partitions$test %>% sdf_predict(fit, .)

summary(fit)

# sparklyr::sdf_register("nameoftable")

```

##==============================================================================
## Evaluate 
##==============================================================================
```{r}

# Plot prediction curve
#df <- data.frame(
#  x = partitions$test,
#  y = collect(sdf_predict(fit, newdata = partitions$test, type = "response"))$prediction  
  #type = "response" #   0 or 1, if you dont write anything it will give the statical value
#)


#ggplot(df, aes(x.criticalFound,y))+
#  geom_point() # +
 # geom_point(data=beaver, aes(temp,as.numeric(activ)-1), colour="red")

scored_data
   

scored_data %>% 
    collect() %>% 
    tidyr::unnest(rawPrediction, probability) %>% 
    mutate(rowid = row_number()) %>% 
    filter(rowid %% 2 == 0 ) ->  # critical found = 0, critical not found =1
    unpacked_data

1:10 %% 2
1:10 %% 3

ggplot(unpacked_data, aes(probability, group = criticalFound, colour = criticalFound)) +
    geom_density()


ggplot(unpacked_data, aes(probability, group = criticalFound, fill = criticalFound)) +
    geom_density(alpha= 0.5)


ggplot(unpacked_data, aes(probability, group = criticalFound, fill = as.factor(criticalFound))) +
           geom_density(alpha= 0.5) +
           theme(legend.position = "bottom") +
           #scale_fill_brewer(type = "qual", )
           theme_tufte() +
            scale_fill_colorblind()
                    


```
 


##==============================================================================
## ROC curve
##==============================================================================

```{r}


pROC::roc(unpacked_data$criticalFound, unpacked_data$probability) %>% 
    pROC::ggroc()
    #    ROCR::performance()


```





### Cleanup
```{r}
spark_disconnect(sc)

```


